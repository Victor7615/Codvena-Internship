# -*- coding: utf-8 -*-
"""Part 1: Oil Production Forecasting - Training Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YwZsfyDQS9T_1FFcXl1CtJ2RXkWbGgOw
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
import mlflow
import mlflow.lightgbm
from mlflow.models.signature import infer_signature
import matplotlib.pyplot as plt
import seaborn as sns
import os

# --- Configuration ---
# File path for the dataset
DATA_FILE_PATH = "Volve production data.xlsx - Daily Production Data.csv"
# Choose a specific well to model. 'NO 15/9-F-1 C' has a good amount of data.
WELL_TO_MODEL = 'NO 15/9-F-1 C'
# Target variable for prediction
TARGET_VARIABLE = 'BORE_OIL_VOL'
# MLflow experiment name
MLFLOW_EXPERIMENT_NAME = "Oil_Production_Forecasting_Volve"

def load_and_prepare_data(file_path, well_code):
    """
    Loads the raw production data, filters for a specific well,
    and performs initial cleaning and type conversion.

    Args:
        file_path (str): The path to the CSV data file.
        well_code (str): The code of the well to filter for.

    Returns:
        pd.DataFrame: A cleaned and filtered DataFrame ready for feature engineering.
    """
    print("--- Loading and Preparing Data ---")
    try:
        df = pd.read_csv(file_path, low_memory=False)
        print(f"Successfully loaded data with shape: {df.shape}")
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return None

    # Filter for the specific well
    df_well = df[df['WELL_BORE_CODE'] == well_code].copy()
    print(f"Filtered for well '{well_code}'. Shape: {df_well.shape}")

    # --- Data Cleaning ---
    # Convert DATEPRD to datetime and set as index
    df_well['DATEPRD'] = pd.to_datetime(df_well['DATEPRD'])
    df_well.set_index('DATEPRD', inplace=True)
    df_well.sort_index(inplace=True)

    # Select relevant numeric columns for the model
    # These are potential features that could influence oil production
    relevant_cols = [
        'ON_STREAM_HRS', 'AVG_DOWNHOLE_PRESSURE', 'AVG_DOWNHOLE_TEMPERATURE',
        'AVG_DP_TUBING', 'AVG_ANNULUS_PRESS', 'AVG_CHOKE_SIZE_P', 'AVG_WHP_P',
        'AVG_WHT_P', 'BORE_GAS_VOL', 'BORE_WAT_VOL', TARGET_VARIABLE
    ]
    df_well = df_well[relevant_cols]

    # Convert all selected columns to numeric, coercing errors to NaN
    for col in df_well.columns:
        df_well[col] = pd.to_numeric(df_well[col], errors='coerce')

    # Impute missing values using forward fill, which is suitable for time series
    # This carries the last known value forward.
    df_well.fillna(method='ffill', inplace=True)
    # Backfill to handle any NaNs at the beginning
    df_well.fillna(method='bfill', inplace=True)

    print("Data cleaning and type conversion complete.")
    print(f"Final shape after cleaning: {df_well.shape}")
    print(f"Missing values remaining: {df_well.isnull().sum().sum()}")
    return df_well

def feature_engineering(df):
    """
    Creates time-based and lag features for the model.

    Args:
        df (pd.DataFrame): The input DataFrame with a datetime index.

    Returns:
        pd.DataFrame: DataFrame with new engineered features.
    """
    print("\n--- Performing Feature Engineering ---")
    df_feat = df.copy()

    # Time-based features
    df_feat['year'] = df_feat.index.year
    df_feat['month'] = df_feat.index.month
    df_feat['dayofyear'] = df_feat.index.dayofyear
    df_feat['dayofweek'] = df_feat.index.dayofweek

    # Lag features for the target variable
    # This helps the model understand the auto-regressive nature of production
    for lag in [1, 7, 14]:
        df_feat[f'{TARGET_VARIABLE}_lag_{lag}'] = df_feat[TARGET_VARIABLE].shift(lag)

    # Rolling window features for the target variable
    # This captures trends and volatility over different periods
    for window in [7, 14, 30]:
        df_feat[f'{TARGET_VARIABLE}_roll_mean_{window}'] = df_feat[TARGET_VARIABLE].rolling(window=window).mean()
        df_feat[f'{TARGET_VARIABLE}_roll_std_{window}'] = df_feat[TARGET_VARIABLE].rolling(window=window).std()

    # Drop NaNs created by lags and rolling windows
    df_feat.dropna(inplace=True)

    print("Feature engineering complete.")
    print(f"Shape after feature engineering: {df_feat.shape}")
    print("Features created:", [col for col in df_feat.columns if col not in df.columns])
    return df_feat

def train_and_log_model(df):
    """
    Splits data, trains a LightGBM model with hyperparameter tuning,
    and logs everything to MLflow.

    Args:
        df (pd.DataFrame): The full feature-engineered DataFrame.
    """
    print("\n--- Model Training and Logging ---")

    # Split data into features (X) and target (y)
    X = df.drop(TARGET_VARIABLE, axis=1)
    y = df[TARGET_VARIABLE]

    # Chronological train-test split for time series
    split_date = X.index.max() - pd.DateOffset(months=6)
    X_train, X_test = X.loc[X.index <= split_date], X.loc[X.index > split_date]
    y_train, y_test = y.loc[y.index <= split_date], y.loc[y.index > split_date]

    print(f"Training data shape: {X_train.shape}")
    print(f"Test data shape: {X_test.shape}")

    # Set up MLflow
    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
    with mlflow.start_run() as run:
        print(f"MLflow Run Started. Run ID: {run.info.run_id}")
        mlflow.log_param("well_code", WELL_TO_MODEL)
        mlflow.log_param("target_variable", TARGET_VARIABLE)

        # --- Hyperparameter Tuning with GridSearchCV ---
        # TimeSeriesSplit is crucial for valid cross-validation
        tscv = TimeSeriesSplit(n_splits=3)

        # LightGBM model
        lgbm = lgb.LGBMRegressor(random_state=42)

        # Define a parameter grid for tuning
        param_grid = {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'num_leaves': [20, 31],
            'max_depth': [-1, 10]
        }

        print("Starting GridSearchCV for hyperparameter tuning...")
        grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=tscv,
                                   scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)
        grid_search.fit(X_train, y_train)

        best_params = grid_search.best_params_
        print(f"Best parameters found: {best_params}")
        mlflow.log_params(best_params)

        # Train final model with best parameters
        final_model = lgb.LGBMRegressor(**best_params, random_state=42)
        final_model.fit(X_train, y_train)

        # --- Evaluation ---
        predictions = final_model.predict(X_test)
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))

        print(f"Test Set MAE: {mae:.2f}")
        print(f"Test Set RMSE: {rmse:.2f}")
        mlflow.log_metric("test_mae", mae)
        mlflow.log_metric("test_rmse", rmse)

        # --- Log Artifacts ---
        # 1. Feature Importance Plot
        fig, ax = plt.subplots(figsize=(12, 8))
        lgb.plot_importance(final_model, ax=ax, max_num_features=20)
        plt.title('Feature Importance')
        plt.tight_layout()

        # Save plot to a temporary file to log as an artifact
        plot_path = "feature_importance.png"
        fig.savefig(plot_path)
        mlflow.log_artifact(plot_path, "plots")
        os.remove(plot_path) # Clean up the temp file
        print("Logged feature importance plot.")

        # 2. Model Signature and Model Logging
        signature = infer_signature(X_train, final_model.predict(X_train))
        mlflow.lightgbm.log_model(
            lgb_model=final_model,
            artifact_path="model",
            signature=signature,
            registered_model_name=f"oil-prod-forecast-{WELL_TO_MODEL.replace(' ', '_')}"
        )
        print("Logged model with signature.")
        print(f"\n--- Model training complete and logged to MLflow experiment '{MLFLOW_EXPERIMENT_NAME}' ---")


if __name__ == '__main__':
    # Execute the pipeline
    df_cleaned = load_and_prepare_data(DATA_FILE_PATH, WELL_TO_MODEL)
    if df_cleaned is not None:
        df_featured = feature_engineering(df_cleaned)
        train_and_log_model(df_featured)
        print("\nPipeline finished successfully.")
        print("To view the results, run 'mlflow ui' in your terminal and navigate to the experiment.")